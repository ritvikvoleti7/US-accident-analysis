{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf82e1e3",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd49eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install swifter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3c25a",
   "metadata": {},
   "source": [
    "# Collecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad69a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv('US_Accidents_Dec21_updated.csv')\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376411d8",
   "metadata": {},
   "source": [
    "# Dropping unnecessary/empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy(deep=True)\n",
    "del data['Country']\n",
    "del data['Turning_Loop']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406a5e5",
   "metadata": {},
   "source": [
    "# Checking imbalance in dataset for Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph and setting the figure size.\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "\n",
    "severity_norm = data['Severity'].value_counts(normalize=True)\n",
    "\n",
    "plt.bar(severity_norm.index, severity_norm.values)\n",
    "plt.xlabel(\"Severity\")\n",
    "plt.ylabel(\"Normalized frequency\")\n",
    "plt.title(\"Types of severity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957a90a",
   "metadata": {},
   "source": [
    "# Distribution of Wind Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17709d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Wind_Direction'].value_counts().plot.bar();\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 6]\n",
    "plt.xlabel(\"Wind Directions\");\n",
    "plt.ylabel(\"Frequency\");\n",
    "plt.title(\"Types of directions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1c5ec",
   "metadata": {},
   "source": [
    "# Distribution of Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(16,7))\n",
    "data['Weather_Condition'].value_counts().sort_values(ascending=False).head(5).plot.bar(width=0.5,edgecolor='k',align='center',linewidth=2)\n",
    "plt.xlabel('Weather_Condition',fontsize=20)\n",
    "plt.ylabel('Number of Accidents',fontsize=20)\n",
    "ax.tick_params(labelsize=20)\n",
    "plt.title('Top Weather Conditions for accidents',fontsize=25)\n",
    "plt.grid()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df0757",
   "metadata": {},
   "source": [
    "# State-wise distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.read_excel('states.xlsx')\n",
    "states = states.drop(columns=['state'])\n",
    "dictionary_pop = states.set_index('code')['pop_2014'].to_dict()\n",
    "df_freq = pd.DataFrame(original_data.groupby('State').size())\n",
    "dictionary_freq = dict()\n",
    "for s in list(df_freq.index):\n",
    "    dictionary_freq[s] = df_freq.loc[s][0]\n",
    "    \n",
    "dict_final = dict()\n",
    "for k in dictionary_freq.keys():\n",
    "    dict_final[k] = dictionary_freq[k]/dictionary_pop[k]\n",
    "    \n",
    "x_values = list(dict_final.keys())\n",
    "y_values = list(dict_final.values())\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.bar(x_values,y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7ed4d",
   "metadata": {},
   "source": [
    "# Roll-up operations on City, County, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c68b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_up(df, col):\n",
    "    level=-1\n",
    "    if col=='Country':\n",
    "        level=4\n",
    "    elif col=='State':\n",
    "        level=3\n",
    "    elif col=='County':\n",
    "        level=2\n",
    "    elif col=='City':\n",
    "        level=1\n",
    "\n",
    "    df = df.iloc[:,level-1:]\n",
    "    cols_to_group = df.columns[:1]\n",
    "    cols_to_agg = df.columns[1:]\n",
    "    rolled_up_df = df.groupby(list(cols_to_group))\n",
    "    rolled_up_df = rolled_up_df[cols_to_agg].mean().reset_index()\n",
    "    return rolled_up_df\n",
    "\n",
    "rolled_up_df = roll_up(df, 'County')\n",
    "print(len(rolled_up_df))\n",
    "print(rolled_up_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee5ecd",
   "metadata": {},
   "source": [
    "# Kernel Distribution Estimate Plot of Humidity vs Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678cf89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data, x=\"Humidity(%)\", hue=\"Severity\",kind='kde', palette=\"Set1\", height=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d211d",
   "metadata": {},
   "source": [
    "# Calculating Duration of Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8991714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def duration(row):\n",
    "    s1 = row['Start_Time']\n",
    "    s2 = row['End_Time']\n",
    "    if '.' in s1:\n",
    "        i = s1.index('.')\n",
    "        s1 = s1[:i]\n",
    "        \n",
    "    if '.' in s2:\n",
    "        i = s2.index('.')\n",
    "        s2 = s2[:i]\n",
    "        \n",
    "    start = datetime.strptime(s1,'%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.strptime(s2,'%Y-%m-%d %H:%M:%S')\n",
    "    return end-start\n",
    "\n",
    "\n",
    "data['Duration'] = data[['Start_Time','End_Time']].swifter.apply(duration,axis=1)\n",
    "data['Duration'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a9cc8",
   "metadata": {},
   "source": [
    "## Extracting hours from duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ed875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hours(td):\n",
    "    return td.total_seconds()/3600\n",
    "\n",
    "data['Duration_hours'] = data['Duration'].swifter.apply(convert_hours)\n",
    "data['Duration_hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc27f2",
   "metadata": {},
   "source": [
    "## Plotting Durations and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97da710",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "values, bins, bars = plt.hist(exp4['Duration_hours'], bins=[0,1,2,3,4,5,6,7,8,9,10],edgecolor='white')\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title = ('Hours/Frequency')\n",
    "plt.bar_label(bars, fontsize=20, color='navy')\n",
    "plt.margins(x=0.01, y=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bfbb3",
   "metadata": {},
   "source": [
    "Notes about sorted_hours:\n",
    "\n",
    "Total records: 2845342\n",
    "\n",
    "1. Records<1402: 2844158 >1402: 1183 (99.96%)\n",
    "2. Records<70: 2841284 >70: 2874 (99.86%)\n",
    "3. Records<7: 2697063 >7: 143264 (94.79%)\n",
    "4. Records<1: 618632 >1: 1591783 (21.74%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a928b4",
   "metadata": {},
   "source": [
    "# Monthly distribution of accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month(d):\n",
    "    return d[5:7]\n",
    "data['Month'] = data['Start_Time'].swifter.apply(month)\n",
    "d = data['Month'].value_counts().to_dict()\n",
    "months = list(d.keys())\n",
    "frequencies = list(d.values())\n",
    "values = []\n",
    "for k in sorted(d):\n",
    "    values.append(d[k])\n",
    "    \n",
    "plt.bar(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'],values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826acde",
   "metadata": {},
   "source": [
    "# Yearly increase in accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8385a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year(d):\n",
    "    return d[:4]\n",
    "\n",
    "data['Year'] = data['Start_Time'].swifter.apply(year)\n",
    "\n",
    "exp = data[['Year','Month']]\n",
    "exp = exp.sort_values(['Year', 'Month'],\n",
    "              ascending = [True, True])\n",
    "\n",
    "grouped = exp.groupby(['Year','Month']).size()\n",
    "time_index = list(grouped.index)\n",
    "l = list(grouped[:])\n",
    "\n",
    "plt.plot(l)\n",
    "\n",
    "# [('2016','01'),('2016','12'),('2017','01'),('2017','12'),('2018','01'),('2018','12'),('2019','01'),('2019','12'),('2020','01'),('2020','12'),('2021','01'),('2021','12')]\n",
    "\n",
    "for i in range(len(l)):\n",
    "    if time_index[i] in [('2016','01'),('2017','01'),('2018','01'),('2019','01'),('2020','01'),('2021','01')]:\n",
    "        plt.text(i,l[i],time_index[i][0])\n",
    "        plt.scatter(i,l[i])\n",
    "        \n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec0d82f",
   "metadata": {},
   "source": [
    "# Hourly Distribution of accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df = data[['Start_Time']]\n",
    "\n",
    "def extract_hours(s):\n",
    "    return int(s[11:13])\n",
    "\n",
    "hourly_df['Hour'] = hourly_df['Start_Time'].swifter.apply(extract_hours)\n",
    "grouped_hrs = hourly_df.groupby(['Hour']).size()\n",
    "plt.plot(grouped_hrs)\n",
    "plt.grid()\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks([i for i in range(len(grouped_hrs))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a1603",
   "metadata": {},
   "source": [
    "# Calculating latitude and longitude of accident by averaging start and end latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75becc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_latlng(row):\n",
    "    lat = (row['Start_Lat']+row['End_Lat'])/2\n",
    "    lng = (row['Start_Lng']+row['End_Lng'])/2\n",
    "    return lat,lng\n",
    "\n",
    "combo = data[['Start_Lat','End_Lat','Start_Lng','End_Lng']].swifter.apply(calc_latlng,axis=1)\n",
    "latitudes = []\n",
    "longitudes = []\n",
    "\n",
    "for item in combo:\n",
    "    latitudes.append(item[0])\n",
    "    longitudes.append(item[1])\n",
    "    \n",
    "print(latitudes)\n",
    "print(longitudes)\n",
    "\n",
    "data['Latitude'] = latitudes\n",
    "data['Longitude'] = longitudes\n",
    "\n",
    "data[['Latitude','Longitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d1e0d",
   "metadata": {},
   "source": [
    "# Global accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datashader\n",
    "import datashader as ds, pandas as pd, colorcet as cc\n",
    "import holoviews as hv\n",
    "from holoviews.element.tiles import EsriImagery,EsriUSATopo,EsriTerrain,CartoMidnight,StamenWatercolor,StamenTonerBackground\n",
    "from holoviews.operation.datashader import datashade\n",
    "from holoviews.element import tiles as hvts\n",
    "\n",
    "#longitudes,latitudes = ds.utils.lnglat_to_meters(data['Longitude'],data['Latitude'])\n",
    "hv.extension('bokeh')\n",
    "\n",
    "map_tiles  = EsriImagery().opts(alpha=0.5, width=900, height=600, bgcolor='black')\n",
    "points = hv.Points(ds.utils.lnglat_to_meters(data['Longitude'], data['Latitude']))\n",
    "hvts.StamenLabels().options(level='annotation', alpha=1)\n",
    "print(points)\n",
    "\n",
    "us_accidents = datashade(points, x_sampling=1, y_sampling=1, cmap=cc.bmw, width=900, height=600)\n",
    "\n",
    "map_tiles * us_accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbbbe28",
   "metadata": {},
   "source": [
    "# Precipitation effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = data[data['Precipitation(in)']!=0.0]\n",
    "\n",
    "plt.hist(prec['Precipitation(in)'],bins=[0,0.01,0.02,0.03,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8])\n",
    "plt.xlabel('Precipitation(inches)')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5f940",
   "metadata": {},
   "source": [
    "# Taking Numerical columns for predicting Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = data[['Distance(mi)',\n",
    "       'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)',\n",
    "       'Visibility(mi)', 'Wind_Speed(mph)',\n",
    "       'Precipitation(in)', 'Sunrise_Sunset', 'Civil_Twilight',\n",
    "       'Nautical_Twilight', 'Astronomical_Twilight']]\n",
    "\n",
    "def daynight(s):\n",
    "    if s=='Night':\n",
    "        return 0\n",
    "    elif s=='Day':\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "numerical_data['Sunrise_Sunset'] = numerical_data['Sunrise_Sunset'].swifter.apply(daynight)\n",
    "numerical_data['Civil_Twilight'] = numerical_data['Civil_Twilight'].swifter.apply(daynight)\n",
    "numerical_data['Nautical_Twilight'] = numerical_data['Nautical_Twilight'].swifter.apply(daynight)\n",
    "numerical_data['Astronomical_Twilight'] = numerical_data['Astronomical_Twilight'].swifter.apply(daynight)\n",
    "\n",
    "for col in ['Distance(mi)',\n",
    "       'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)',\n",
    "       'Visibility(mi)', 'Wind_Speed(mph)',\n",
    "       'Precipitation(in)', 'Sunrise_Sunset', 'Civil_Twilight',\n",
    "       'Nautical_Twilight', 'Astronomical_Twilight']:\n",
    "    \n",
    "    \n",
    "    numerical_data[col] = numerical_data[col].fillna(numerical_data[col].mean())\n",
    "\n",
    "target = data['Severity']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    numerical_data, target, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e1ede",
   "metadata": {},
   "source": [
    "# Case 1: Predicting Severity using all numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71bf80",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf_base = RandomForestClassifier()\n",
    "grid = {'n_estimators': [10, 50, 100],\n",
    "        'max_features': ['auto','sqrt']}\n",
    "clf_rf_base = GridSearchCV(rf_base, grid, cv=5, n_jobs=8, scoring='f1_macro')\n",
    "\n",
    "clf_rf_base.fit(x_train, y_train)\n",
    "y_pred_rf_base = clf_rf_base.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_rf_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a881a",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_base = LogisticRegression()\n",
    "\n",
    "# log_base = GridSearchCV(log_base,cv=5, n_jobs=8, scoring='f1_macro')\n",
    "\n",
    "log_base.fit(x_train, y_train)\n",
    "y_pred_log_base = log_base.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_log_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5316506c",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_base = SVC(gamma='auto')\n",
    "\n",
    "svm_base.fit(x_train, y_train)\n",
    "y_pred_svm_base = svm_base.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_svm_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e5882",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred_knn_3 = knn.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_knn_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6d096",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ad6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_base = GaussianNB()\n",
    "\n",
    "nb_base.fit(x_train, y_train)\n",
    "y_pred_nb_base = nb_base.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_nb_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3e2c5",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f57472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_15_5 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15, 5), random_state=1)\n",
    "\n",
    "mlp_15_5.fit(x_train, y_train)\n",
    "y_pred_mlp_15_5 = mlp_15_5.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_mlp_15_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44d43c",
   "metadata": {},
   "source": [
    "# Checking correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc03201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "hm = numerical_data.corr().abs()\n",
    "hm[hm>0.8]\n",
    "sns.heatmap(hm[hm>0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07d1c3",
   "metadata": {},
   "source": [
    "# Removing correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing columns that are correlated by more than 80%\n",
    "numerical_data = numerical_data.drop(columns=['Wind_Chill(F)','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32304ae6",
   "metadata": {},
   "source": [
    "# Case 2: Predicting Severity using un-correlated columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf3283",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf_base = RandomForestClassifier()\n",
    "grid = {'n_estimators': [10, 50, 100],\n",
    "        'max_features': ['auto','sqrt']}\n",
    "clf_rf_base = GridSearchCV(rf_base, grid, cv=5, n_jobs=8, scoring='f1_macro')\n",
    "\n",
    "clf_rf_base.fit(x_train, y_train)\n",
    "y_pred_rf_base = clf_rf_base.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_rf_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31186a",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd81f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_base = LogisticRegression()\n",
    "\n",
    "# log_base = GridSearchCV(log_base,cv=5, n_jobs=8, scoring='f1_macro')\n",
    "\n",
    "log_base.fit(x_train, y_train)\n",
    "y_pred_log_base = log_base.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_log_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df8a390",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_base = SVC(gamma='auto')\n",
    "\n",
    "svm_base.fit(x_train, y_train)\n",
    "y_pred_svm_base = svm_base.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_svm_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9442c74",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred_knn_3 = knn.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_knn_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1503040",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33134380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_base = GaussianNB()\n",
    "\n",
    "nb_base.fit(x_train, y_train)\n",
    "y_pred_nb_base = nb_base.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_nb_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729059a",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_15_5 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15, 5), random_state=1)\n",
    "\n",
    "mlp_15_5.fit(x_train, y_train)\n",
    "y_pred_mlp_15_5 = mlp_15_5.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_mlp_15_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecc2d4",
   "metadata": {},
   "source": [
    "# Case 3: Predicting Severity using un-correlated columns as well as Side and Timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afa6fc",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf_base = RandomForestClassifier()\n",
    "grid = {'n_estimators': [10, 50, 100],\n",
    "        'max_features': ['auto','sqrt']}\n",
    "clf_rf_base = GridSearchCV(rf_base, grid, cv=5, n_jobs=8, scoring='f1_macro')\n",
    "\n",
    "clf_rf_base.fit(x_train, y_train)\n",
    "y_pred_rf_base = clf_rf_base.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_rf_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0b911",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_base = LogisticRegression()\n",
    "\n",
    "# log_base = GridSearchCV(log_base,cv=5, n_jobs=8, scoring='f1_macro')\n",
    "\n",
    "log_base.fit(x_train, y_train)\n",
    "y_pred_log_base = log_base.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_log_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9065b",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d754dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_base = SVC(gamma='auto')\n",
    "\n",
    "svm_base.fit(x_train, y_train)\n",
    "y_pred_svm_base = svm_base.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_svm_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e779e4",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae489ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred_knn_3 = knn.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_knn_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76dc657",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf48172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_base = GaussianNB()\n",
    "\n",
    "nb_base.fit(x_train, y_train)\n",
    "y_pred_nb_base = nb_base.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_nb_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53b45d",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_15_5 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15, 5), random_state=1)\n",
    "\n",
    "mlp_15_5.fit(x_train, y_train)\n",
    "y_pred_mlp_15_5 = mlp_15_5.predict(x_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred_mlp_15_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69bd706",
   "metadata": {},
   "source": [
    "# Conducting statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb337049",
   "metadata": {},
   "source": [
    "## Friedman's chi2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53339a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "import numpy as np\n",
    "\n",
    "#List of accuracy scores for four different models taken from the accuracies obtained above\n",
    "rf_acc = [0.76, 0.90, 0.91]\n",
    "log_acc = [0.73, 0.89, 0.89]\n",
    "mlp_acc = [0.65,0.86,0.80]\n",
    "nb_acc = [0.70,0.72,0.89]\n",
    "\n",
    "# Concatenate the accuracy scores for each model into a 2D array\n",
    "accuracy_scores = np.concatenate([rf_acc, log_acc, nb_acc, mlp_acc]).reshape(4,3)\n",
    "\n",
    "# Perform Friedman's test\n",
    "f_value, p_value = friedmanchisquare(*accuracy_scores)\n",
    "\n",
    "print(\"Friedman's test statistic:\", f_value)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "#model_scores = [0.76,0.73,0.74,0.65,0.70,0.73]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed987fa",
   "metadata": {},
   "source": [
    "## Kruskal-Wallis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369eb878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "kw_stat, p_value = kruskal(*accuracy_scores)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5135f",
   "metadata": {},
   "source": [
    "## ANOVA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "f_stat, p_value = f_oneway(*accuracy_scores)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e4b96",
   "metadata": {},
   "source": [
    "## Bon-ferroni test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f02b3f",
   "metadata": {},
   "source": [
    "### Taking accuracies from different cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "rf_acc = [0.76, 0.90, 0.91]\n",
    "log_acc = [0.73, 0.89, 0.89]\n",
    "mlp_acc = [0.65,0.86,0.80]\n",
    "nb_acc = [0.70,0.72,0.89]\n",
    "\n",
    "case1 = [0.76,0.73,0.65,0.70] #corresponds to the case where we use all numerical columns\n",
    "case2 = [0.9,0.89,0.86,0.72] #corresponds to the case where we use un-correlated columns\n",
    "case3 = [0.91,0.89,0.8,0.89] #corresponds to the case where we use un-correlated columns as well as two more categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb00e89",
   "metadata": {},
   "source": [
    "### Case 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "p_values = [1 - model_score for model_score in case1]\n",
    "reject, corrected_p_values, _, _ = multipletests(p_values, alpha=alpha, method='bonferroni')\n",
    "corrected_p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd337da0",
   "metadata": {},
   "source": [
    "### Case 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "p_values = [1 - model_score for model_score in case2]\n",
    "reject, corrected_p_values, _, _ = multipletests(p_values, alpha=alpha, method='bonferroni')\n",
    "corrected_p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38116164",
   "metadata": {},
   "source": [
    "### Case 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "p_values = [1 - model_score for model_score in case3]\n",
    "reject, corrected_p_values, _, _ = multipletests(p_values, alpha=alpha, method='bonferroni')\n",
    "corrected_p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34bd9a7",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing to predict severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "rel = data[['Description','Severity']]\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re,nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "rel['Mod1'] = rel['Description'].apply(lambda x: x.lower())\n",
    "rel['Mod2'] = rel['Mod1'].apply(lambda x: re.sub('[^a-zA-Z\\s]', '', x))\n",
    "rel['Mod3'] = rel['Mod2'].apply(lambda x: x.split())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "rel['Mod4'] = rel['Mod3'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "rel['Mod5'] = rel['Mod4'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "\n",
    "# Fit and transform the text data to a TF-IDF representation\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(rel['Mod5'])\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(tfidf_matrix,rel['Severity'], test_size=0.2, random_state=22)\n",
    "X_train = X_train.todense()\n",
    "X_test = X_test.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e1f77",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Train the model on the TF-IDF matrix and the corresponding labels\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new text data\n",
    "\n",
    "# Transform the new text data to a TF-IDF representation\n",
    "#new_tfidf_matrix = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Make predictions on the new TF-IDF matrix\n",
    "predictions = logreg_model.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(accuracy_score(predictions,y_test))\n",
    "\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da86e5",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bf310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the TF-IDF matrix and the corresponding labels\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new text data\n",
    "\n",
    "# Transform the new text data to a TF-IDF representation\n",
    "#new_tfidf_matrix = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Make predictions on the new TF-IDF matrix\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "#print(rf_predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(accuracy_score(rf_predictions,y_test))\n",
    "\n",
    "print(classification_report(rf_predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778c4c9",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Random Forest model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the model on the TF-IDF matrix and the corresponding labels\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new text data\n",
    "\n",
    "# Transform the new text data to a TF-IDF representation\n",
    "#new_tfidf_matrix = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Make predictions on the new TF-IDF matrix\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "#print(rf_predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(accuracy_score(nb_predictions,y_test))\n",
    "\n",
    "print(classification_report(nb_predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a2cbe",
   "metadata": {},
   "source": [
    "# K-Means Clustering on Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd27373",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = data.copy(deep=True)\n",
    "clust = clust.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "# Extract the 'Start_Lat' and 'Start_Lng' columns\n",
    "X = clust[['Latitude', 'Longitude']]\n",
    "\n",
    "# Determine the optimal number of clusters using the elbow method\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Fit the k-means model to the data with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[y_kmeans == 0]['Longitude'],X[y_kmeans == 0]['Latitude'], c = 'red', label = 'Cluster 1',s=1)\n",
    "plt.scatter(X[y_kmeans == 1]['Longitude'],X[y_kmeans == 1]['Latitude'], c = 'blue', label = 'Cluster 2',s=1)\n",
    "plt.scatter(X[y_kmeans == 2]['Longitude'],X[y_kmeans == 2]['Latitude'], c = 'green', label = 'Cluster 3',s=1)\n",
    "plt.scatter(X[y_kmeans == 3]['Longitude'],X[y_kmeans == 3]['Latitude'], c = 'cyan', label = 'Cluster 4',s=1)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 1],kmeans.cluster_centers_[:, 0], c = 'yellow', label = 'Centroids',s=5)\n",
    "plt.title('Clusters of Accidents')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae021bc",
   "metadata": {},
   "source": [
    "# Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv('Checkpoint1.csv')\n",
    "df['Severity'] = df['Severity'].astype(str)\n",
    "# Select the accident features to consider\n",
    "features = ['Traffic_Signal', 'Crossing', 'Junction', 'Stop', 'Amenity', 'Bump', 'Give_Way', 'No_Exit', 'Railway', 'Station','Severity']\n",
    "\n",
    "# Convert the features to binary indicators\n",
    "df_features = pd.get_dummies(df[features])\n",
    "\n",
    "# Find frequent combinations of features using the Apriori algorithm\n",
    "frequent_itemsets = apriori(df_features, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generate association rules between the frequent itemsets\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6277d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364bfef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
